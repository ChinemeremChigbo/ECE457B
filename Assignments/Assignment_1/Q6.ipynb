{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Q6.csv\")\n",
    "\n",
    "X = df[[\"0\", \"1\"]]\n",
    "y = df[\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_threshold = 0\n",
    "val_accuracy_scores = {model_name: None for model_name in models.keys()}\n",
    "precision_scores = {model_name: None for model_name in models.keys()}\n",
    "f1_scores = {model_name: None for model_name in models.keys()}\n",
    "confusion_matrices = {model_name: None for model_name in models.keys()}\n",
    "per_class_accuracies = {model_name: [] for model_name in models.keys()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    if name == \"Linear Regression\":\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_val_pred = (y_val_pred > linear_regression_threshold).astype(int)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred = (y_test_pred > linear_regression_threshold).astype(int)\n",
    "    else:\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_accuracy_scores[name] = val_accuracy\n",
    "    confusion_matrices[name] = confusion_matrix(y_val, y_val_pred)\n",
    "    f1_scores[name] = f1_score(y_test, y_test_pred)\n",
    "    for class_label in np.unique(y_val):\n",
    "        class_indices = y_val == class_label\n",
    "        class_accuracy = accuracy_score(y_val[class_indices], y_val_pred[class_indices])\n",
    "        per_class_accuracies[name].append(class_accuracy)\n",
    "\n",
    "print(\"Validation Accuracy Scores:\")\n",
    "for name, score in val_accuracy_scores.items():\n",
    "    print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a problem because in many real-world classification problems, the classes are not evenly distributed. For example, in fraud detection, disease diagnosis, or rare event detection, the majority of samples may belong to one class, while the other class represents a minority. In such cases, accuracy can be misleading because a model might achieve high accuracy by simply predicting the majority class for all instances, ignoring the minority class completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of classes:\", df[\"2\"].nunique())\n",
    "print(\"Class distribution:\")\n",
    "print(df[\"2\"].value_counts())\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = [\"blue\", \"red\"]\n",
    "for class_label, color in zip(df[\"2\"].unique(), colors):\n",
    "    class_data = df[df[\"2\"] == class_label]\n",
    "    plt.scatter(\n",
    "        class_data[\"0\"], class_data[\"1\"], color=color, label=f\"Class {class_label}\"\n",
    "    )\n",
    "\n",
    "plt.title(\"Scatterplot of the Dataset\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of two classes. The class distribution reveals a significant class imbalance, with Class 0 comprising 9,900 samples and Class 1 containing only 100 samples. This imbalance suggests that Class 0 is heavily overrepresented compared to Class 1, which could potentially pose challenges in training machine learning models, particularly those sensitive to class distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, accuracies in per_class_accuracies.items():\n",
    "    print(f\"Per-class accuracies for {name}:\")\n",
    "    for i, class_accuracy in enumerate(accuracies):\n",
    "        print(f\"Class {i}: {class_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, matrix in confusion_matrices.items():\n",
    "    print(f\"Confusion matrix for {name}:\")\n",
    "    print(matrix)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6If"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision and recall are valuable in the presence of data imbalances because they provide insights into how well a model performs for each class, considering false positives and false negatives:\n",
    "\n",
    "Precision measures the proportion of correctly predicted positive instances among all instances predicted as positive. It helps assess the model's ability to avoid falsely labeling negative instances as positive, crucial when misclassifying negative instances has a high cost.\n",
    "\n",
    "Recall measures the proportion of correctly predicted positive instances among all actual positive instances. It evaluates the model's ability to capture positive instances from the entire population of positive instances, essential for minimizing false negatives and capturing all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, score in f1_scores.items():\n",
    "    print(f\"F1 score for {name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = y_train.value_counts()\n",
    "ratio = class_counts[0] / class_counts[1]\n",
    "\n",
    "accuracy_results = []\n",
    "precision_results = []\n",
    "jump_percent = 0.2\n",
    "jump = int(ratio * jump_percent) if int(ratio * jump_percent) else 1\n",
    "\n",
    "i = 1\n",
    "while class_counts[0] > class_counts[1] * i:\n",
    "    current_ratio = class_counts[0] / (class_counts[1] * i)\n",
    "    print(f\"Current ratio: {round(current_ratio, 3)}\")\n",
    "    oversampler = RandomOverSampler(\n",
    "        sampling_strategy={0: class_counts[0], 1: class_counts[1] * i}\n",
    "    )\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_resampled, y_resampled)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "\n",
    "    print(\"KNN - Accuracy:\", round(accuracy, 3), \"Precision:\", round(precision, 3))\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_resampled, y_resampled)\n",
    "    y_pred = dt.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\n",
    "        \"Decision Tree - Accuracy:\",\n",
    "        round(accuracy, 3),\n",
    "        \"Precision:\",\n",
    "        round(precision, 3),\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_resampled, y_resampled)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_pred = (y_pred > linear_regression_threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\n",
    "        \"Linear Regression - Accuracy:\",\n",
    "        round(accuracy, 3),\n",
    "        \"Precision:\",\n",
    "        round(precision, 3),\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_resampled, y_resampled)\n",
    "    y_pred = log_reg.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\n",
    "        \"Logistic Regression - Accuracy:\",\n",
    "        round(accuracy, 3),\n",
    "        \"Precision:\",\n",
    "        round(precision, 3),\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "    i += jump\n",
    "\n",
    "\n",
    "best_accuracy = max(accuracy_results)\n",
    "best_precision = max(precision_results)\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Best Precision:\", best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided results, it seems that the Decision Tree model performs\n",
    "better in terms of both accuracy and precision at most sampling ratios. For\n",
    "oversampling, the initial ratio of 93.118 i.e. no oversampling, performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = []\n",
    "precision_results = []\n",
    "\n",
    "i = 1\n",
    "while int(class_counts[0] * 1 / i) > class_counts[1]:\n",
    "    current_ratio = int(class_counts[0] * 1 / i) / (class_counts[1])\n",
    "    print(f\"Current ratio: {round(current_ratio, 3)}\")\n",
    "    undersampler = RandomUnderSampler(\n",
    "        sampling_strategy={0: int(class_counts[0] * 1 / i), 1: class_counts[1]}\n",
    "    )\n",
    "    X_resampled, y_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_resampled, y_resampled)\n",
    "    y_pred = knn.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\"KNN - Accuracy:\", round(accuracy, 3), \"Precision:\", round(precision, 3))\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_resampled, y_resampled)\n",
    "    y_pred = dt.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\n",
    "        \"Decision Tree - Accuracy:\",\n",
    "        round(accuracy, 3),\n",
    "        \"Precision:\",\n",
    "        round(precision, 3),\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_resampled, y_resampled)\n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_pred = (y_pred > linear_regression_threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\n",
    "        \"Linear Regression - Accuracy:\",\n",
    "        round(accuracy, 3),\n",
    "        \"Precision:\",\n",
    "        round(precision, 3),\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    # Train Logistic Regression Classifier\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X_resampled, y_resampled)\n",
    "    y_pred = log_reg.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    confusion = confusion_matrix(y_val, y_pred)\n",
    "    accuracy_results.append(accuracy)\n",
    "    precision_results.append(precision)\n",
    "    print(\n",
    "        \"Logistic Regression - Accuracy:\",\n",
    "        round(accuracy, 3),\n",
    "        \"Precision:\",\n",
    "        round(precision, 3),\n",
    "    )\n",
    "    print(\"Confusion matrix:\")\n",
    "    for matrix in confusion:\n",
    "        print(matrix)\n",
    "    print()\n",
    "\n",
    "    i += jump\n",
    "\n",
    "# Find the best model based on accuracy and precision\n",
    "best_accuracy = max(accuracy_results)\n",
    "best_precision = max(precision_results)\n",
    "\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "print(\"Best Precision:\", best_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided results, it seems that the Decision Tree model performs\n",
    "better in terms of both accuracy and precision at most lower ratios but for\n",
    "higher ratios, Logistic regression performs the best. For\n",
    "undersampling, the initial ratio of 93.118 i.e. no undersampling, performs the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 611a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection of the dataset feature x1 of Class 0 has a few outliers. A good\n",
    "approach to dealing with them would be to identify and remove from the dataset\n",
    "using the z-score for each data point in feature x1, which quantifies how many\n",
    "standard deviations the data point is away from the mean of the feature.\n",
    "Following this, a threshold value is set, say, t=3, indicating that any data point with a z-score greater than 3 or less than -3 is considered an outlier. Subsequently, outliers are identified by filtering out data points that exceed the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6IIb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 3, 5]\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    y_val_pred = knn.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"KNN with k={k}:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers may not be a problem for KNN when:\n",
    "- Outliers are located far away from the majority of data points: If outliers are located in regions of the feature space where there are few or no other data points, they will have minimal impact on the nearest neighbor calculations and, therefore, on the classification decision made by KNN.\n",
    "\n",
    "- Outliers are correctly classified: If outliers are correctly classified by KNN based on the majority of their nearest neighbors, they will not affect the overall accuracy of the algorithm significantly.\n",
    "\n",
    "Outliers may be a problem for KNN when:\n",
    "- Outliers are located near the decision boundary: If outliers are situated close to the decision boundary between two or more classes, they can significantly influence the classification decision of KNN by pulling the decision boundary towards them, leading to misclassification of neighboring data points.\n",
    "\n",
    "- Outliers are incorrectly labeled: If outliers are incorrectly labeled or represent noise in the dataset, they can introduce bias into the KNN algorithm, causing it to make erroneous predictions based on the erroneous labels associated with the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6IIc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [1, 2, 3]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    dt_classifier = DecisionTreeClassifier(max_depth=max_depth, random_state=123123)\n",
    "    dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_val_pred = dt_classifier.predict(X_val)\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "    print(f\"Decision Tree Classifier with max depth = {max_depth}:\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers would not be a problem for KNN when:\n",
    "- They are located far away from the majority of data points.\n",
    "- They do not fall within the neighborhood of any other data points.\n",
    "- Their influence on nearest neighbor calculations is minimal.\n",
    "\n",
    "Outliers would be a problem for KNN when:\n",
    "- They are situated near the decision boundary between different classes.\n",
    "- They distort the neighborhood relationships and pull the decision boundary toward them.\n",
    "- They introduce bias into the classification process by being incorrectly labeled or representing noise in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6IId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are generally less impervious to outliers compared to KNNs because outliers can significantly affect the structure and decision boundaries of the tree, leading to overfitting. KNN, on the other hand, tends to be more robust to outliers as their influence diminishes with an increasing number of neighbors and their impact is localized to the vicinity of the query point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the realm of real-world data, the challenges extend beyond imbalanced datasets and outliers. One significant issue lies in missing values, which are common occurrences and can introduce biases, compromise model effectiveness, and lead to inaccurate predictions if not handled meticulously. Data quality is another essential consideration, encompassing noise, inconsistencies, errors, and duplicates that can significantly impact model accuracy. Furthermore, high-dimensional data can exacerbate overfitting and computational complexity, warranting dimensionality reduction techniques to retain essential information while reducing the feature space. Finally, temporal dependencies in time-series or sequential data must be considered, as overlooking such dependencies can result in erroneous predictions and subpar model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
