{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k, metric=\"euclidean\"):\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        if self.metric == \"euclidean\":\n",
    "            distances = self.euclidean_distance(X_test)\n",
    "        elif self.metric == \"cosine\":\n",
    "            distances = self.cosine_similarity(X_test)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid metric. Choose from 'euclidean' or 'cosine'.\")\n",
    "\n",
    "        y_pred = []\n",
    "        for distance in distances:\n",
    "            nearest_neighbors = np.argsort(distance)[: self.k]\n",
    "            nearest_labels = self.y_train[nearest_neighbors]\n",
    "            pred_label = np.argmax(np.bincount(nearest_labels))\n",
    "            y_pred.append(pred_label)\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def euclidean_distance(self, X_test):\n",
    "        distances = []\n",
    "        for x in X_test:\n",
    "            dist = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "            distances.append(dist)\n",
    "        return np.array(distances)\n",
    "\n",
    "    def cosine_similarity(self, X_test):\n",
    "        similarities = []\n",
    "        for x in X_test:\n",
    "            dot_product = np.dot(self.X_train, x)\n",
    "            norm_product = np.linalg.norm(self.X_train, axis=1) * np.linalg.norm(x)\n",
    "            cosine_sim = dot_product / norm_product\n",
    "            similarities.append(cosine_sim)\n",
    "        return np.array(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Q3.csv\", header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "splits = [(0.8, 0.1, 0.1), (0.34, 0.33, 0.33), (0.25, 0.25, 0.5)]\n",
    "\n",
    "k_values = [1, 3, 5, 11]\n",
    "errors_dict = {\"0.8/0.1/0.1\": [], \"0.34/0.33/0.33\": [], \"0.25/0.25/0.5\": []}\n",
    "custom_euclidean_errors = []\n",
    "custom_cosine_errors = []\n",
    "sklearn_euclidean_errors = []\n",
    "sklearn_cosine_errors = []\n",
    "\n",
    "for train_size, val_size, test_size in splits:\n",
    "    for k_value in k_values:\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "            X, y, train_size=train_size, random_state=42\n",
    "        )\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_temp,\n",
    "            y_temp,\n",
    "            test_size=test_size / (val_size + test_size),\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        knn_euclidean = KNN(k=k_value, metric=\"euclidean\")\n",
    "        knn_euclidean.fit(X_train, y_train)\n",
    "        y_pred_euclidean_custom = knn_euclidean.predict(X_val)\n",
    "        val_error_custom = 1 - accuracy_score(y_val, y_pred_euclidean_custom)\n",
    "        custom_euclidean_errors.append(val_error_custom)\n",
    "\n",
    "        knn_cosine = KNN(k=k_value, metric=\"cosine\")\n",
    "        knn_cosine.fit(X_train, y_train)\n",
    "        y_pred_cosine_custom = knn_cosine.predict(X_val)\n",
    "        val_error_cosine_custom = 1 - accuracy_score(y_val, y_pred_cosine_custom)\n",
    "        custom_cosine_errors.append(val_error_cosine_custom)\n",
    "\n",
    "        knn_sklearn_euclidean = KNeighborsClassifier(\n",
    "            n_neighbors=k_value, metric=\"euclidean\"\n",
    "        )\n",
    "        knn_sklearn_euclidean.fit(X_train, y_train)\n",
    "        y_pred_euclidean_sklearn = knn_sklearn_euclidean.predict(X_val)\n",
    "        val_error_euclidean_sklearn = 1 - accuracy_score(\n",
    "            y_val, y_pred_euclidean_sklearn\n",
    "        )\n",
    "        sklearn_euclidean_errors.append(val_error_euclidean_sklearn)\n",
    "\n",
    "        knn_sklearn_cosine = KNeighborsClassifier(n_neighbors=k_value, metric=\"cosine\")\n",
    "        knn_sklearn_cosine.fit(X_train, y_train)\n",
    "        y_pred_cosine_sklearn = knn_sklearn_cosine.predict(X_val)\n",
    "        val_error_cosine_sklearn = 1 - accuracy_score(y_val, y_pred_cosine_sklearn)\n",
    "        sklearn_cosine_errors.append(val_error_cosine_sklearn)\n",
    "\n",
    "        print(\n",
    "            f\"Validation error for k={k_value} and train/val/test ratio={train_size}/{val_size}/{test_size}: (Sklearn KNN, Euclidean) {round(val_error_euclidean_sklearn, 4)}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Validation error for k={k_value} and train/val/test ratio={train_size}/{val_size}/{test_size}: (Sklearn KNN, Cosine) {round(val_error_cosine_sklearn, 3)}\"\n",
    "        )\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train/validation/test split of 80/10/10 yielded the best performance among the provided ratios for several reasons. Primarily, allocating 80% of the dataset for training ensures that the model has ample data to learn from, facilitating better generalization and predictive performance. With a larger training set, the model can capture more diverse patterns and relationships within the data, resulting in a more robust learned representation. While the validation set comprises only 10% of the data, it still offers a sufficient sample size for evaluating the model's performance and tuning hyperparameters effectively. This balance between training and validation data enables thorough model assessment without sacrificing substantial amounts of training data. Furthermore, the 10% test set provides a reliable measure of the model's generalization performance on unseen data while remaining manageable in size for efficient evaluation. By having a larger training set and separate validation and test sets, the 80/10/10 split mitigates the risk of overfitting, as the model is less likely to memorize noise in the data and instead learns more meaningful patterns. Overall, the 80/10/10 split strikes a favorable balance between data allocation for training, validation, and testing, leading to optimal model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"k=1 split=0.8/0.1/0.1\",\n",
    "    \"k=3 split=0.8/0.1/0.1\",\n",
    "    \"k=5 split=0.8/0.1/0.1\",\n",
    "    \"k=11 split=0.8/0.1/0.1\",\n",
    "    \"k=1 split=0.34/0.33/0.33\",\n",
    "    \"k=3 split=0.34/0.33/0.33\",\n",
    "    \"k=5 split=0.34/0.33/0.33\",\n",
    "    \"k=11 split=0.34/0.33/0.33\",\n",
    "    \"k=1 split=0.25/0.25/0.5\",\n",
    "    \"k=3 split=0.25/0.25/0.5\",\n",
    "    \"k=5 split=0.25/0.25/0.5\",\n",
    "    \"k=11 split=0.25/0.25/0.5\",\n",
    "]\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "rects1 = ax.bar(\n",
    "    x - 1.5 * width, custom_euclidean_errors, width, label=\"Custom Euclidean\"\n",
    ")\n",
    "rects2 = ax.bar(\n",
    "    x - 0.5 * width, sklearn_euclidean_errors, width, label=\"Sklearn Euclidean\"\n",
    ")\n",
    "rects3 = ax.bar(x + 0.5 * width, custom_cosine_errors, width, label=\"Custom Cosine\")\n",
    "rects4 = ax.bar(x + 0.5 * width, sklearn_cosine_errors, width, label=\"Sklearn Cosine\")\n",
    "\n",
    "ax.set_ylabel(\"Validation Error\")\n",
    "ax.set_title(\"Validation Error by Train/Val/Test Ratio and Method\")\n",
    "ax.set_xticks(x, labels, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided graphs, the combination with the largest value of k (k=11)\n",
    "for the KNN and a train/val/test split of 0.25/0.25/0.5 performs the best. It\n",
    "has the lowest Euclidean error (0.2031) and a reasonably low euclidean error (0.396).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
