{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "genre_counts = train_data[\"genre\"].value_counts()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(genre_counts.index, genre_counts.values, color=\"skyblue\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Movie Genres\")\n",
    "plt.xticks(genre_counts.index, genre_counts.index.tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drama and documentary genres are the most common, with 5483 and 4861\n",
    "occurrences, respectively. Comedy is the third most common genre with 3896\n",
    "occurrences.Horror, thriller, and action genres have fewer occurrences compared\n",
    "to drama, documentary, and comedy, with 2104, 1568, and 1312 occurrences,\n",
    "respectively. The distribution of the dataset is imbalanced, with drama and\n",
    "documentary genres dominating the dataset, while horror, thriller, and action\n",
    "genres are underrepresented. This class imbalance can potentially affect the\n",
    "performance of machine learning models, particularly for genres with fewer\n",
    "occurrences, as the model may struggle to generalize well to these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "validation_data = pd.read_csv(\"validate.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=[\"genre\"])\n",
    "y_train = train_data[\"genre\"]\n",
    "X_val = validation_data.drop(columns=[\"genre\"])\n",
    "y_val = validation_data[\"genre\"]\n",
    "X_test = test_data.drop(columns=[\"ID\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_encoded), y=y_train_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, hidden_nodes, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            hidden_nodes, activation=\"relu\", input_shape=(X_train.shape[1],)\n",
    "        )\n",
    "    )\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(hidden_nodes, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(len(label_encoder.classes_), activation=\"softmax\"))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    y_train_pred_data = model.predict(X_train)\n",
    "    y_train_pred = np.argmax(y_train_pred_data, axis=1)\n",
    "    y_val_pred_data = model.predict(X_val)\n",
    "    y_val_pred = np.argmax(y_val_pred_data, axis=1)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "    print(\"Accuracy Scores:\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        print(f\"{genre}: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structures = [\n",
    "    # {\"hidden_layers\": 1, \"hidden_nodes\": 64},\n",
    "    # {\"hidden_layers\": 2, \"hidden_nodes\": 32},\n",
    "    # {\"hidden_layers\": 2, \"hidden_nodes\": 64},\n",
    "]\n",
    "\n",
    "for idx, structure in enumerate(model_structures, start=1):\n",
    "    hidden_layers = structure[\"hidden_layers\"]\n",
    "    hidden_nodes = structure[\"hidden_nodes\"]\n",
    "    learning_rate = 0.001\n",
    "    epochs = 100\n",
    "\n",
    "    print(\n",
    "        f\"\\nModel {idx}: Hidden Layers: {hidden_layers}, Hidden Nodes: {hidden_nodes}\"\n",
    "    )\n",
    "    model = build_model(hidden_layers, hidden_nodes, learning_rate)\n",
    "    history = model.fit(\n",
    "\n",
    "        X_train,\n",
    "        y_train_encoded,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val_encoded),\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    plot_training_history(history, f\"Model {idx}: Training vs Validation Error\")\n",
    "\n",
    "    evaluate_model(model, X_train, y_train_encoded, X_val, y_val_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 with 1 hidden layer and 64 hidden nodes achieves a training accuracy of 0.690 and a validation accuracy of 0.635.\n",
    "Model 2 with 2 hidden layers and 32 hidden nodes achieves a training accuracy of 0.685 and a validation accuracy of 0.647.\n",
    "Model 3 with 2 hidden layers and 64 hidden nodes achieves a training accuracy of 0.712 and a validation accuracy of 0.653.\n",
    "From these performances, we can observe the following:\n",
    "\n",
    "Model 3, which has the highest number of hidden nodes (64) and the best performance, achieves the highest validation accuracy. This suggests that increasing the complexity of the model by adding more hidden nodes can improve performance, up to a certain point.\n",
    "\n",
    "Model 2, with two hidden layers, performs slightly better than Model 1 with only one hidden layer. This indicates that adding depth to the network can also contribute to better performance, although the improvement is not as significant as increasing the number of nodes.\n",
    "\n",
    " While Model 3 achieves the highest training accuracy, its validation accuracy is not significantly higher compared to Model 2, suggesting that Model 3 might be overfitting to the training data. Therefore, it's essential to strike a balance between model complexity and generalization performance to avoid overfitting.\n",
    "\n",
    "In summary, Model 3 with 2 hidden layers and 64 hidden nodes seems to strike a good balance between model complexity and performance, achieving the highest validation accuracy among the three models. However, further tuning and experimentation may be necessary to optimize the model's performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structures = [\n",
    "    {\"hidden_layers\": 4, \"hidden_nodes\": 250, \"learning_rate\": 0.00003},\n",
    "]\n",
    "\n",
    "for idx, structure in enumerate(model_structures, start=1):\n",
    "    hidden_layers = structure[\"hidden_layers\"]\n",
    "    hidden_nodes = structure[\"hidden_nodes\"]\n",
    "    learning_rate = structure[\"learning_rate\"]\n",
    "    epochs = 100\n",
    "\n",
    "    print(\n",
    "        f\"\\nModel {idx}: Hidden Layers: {hidden_layers}, Hidden Nodes: {hidden_nodes}\"\n",
    "    )\n",
    "    model = build_model(hidden_layers, hidden_nodes, learning_rate)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_encoded,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val_encoded),\n",
    "        verbose=1,\n",
    "        class_weight=dict(enumerate(class_weights)),\n",
    "    )\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    submission_df = pd.DataFrame(\n",
    "        {\"ID\": test_data[\"ID\"], \"label\": np.argmax(predictions, axis=1) + 1}\n",
    "    )\n",
    "\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    plot_training_history(history, f\"Model {idx}: Training vs Validation Error\")\n",
    "\n",
    "    evaluate_model(model, X_train, y_train_encoded, X_val, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(hidden_layers, hidden_nodes):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     # Input layer\n",
    "#     model.add(\n",
    "#         tf.keras.layers.Dense(hidden_nodes, activation=\"relu\", input_shape=(input_shape,))\n",
    "#     )\n",
    "#     # Hidden layers\n",
    "#     for _ in range(hidden_layers - 1):\n",
    "#         model.add(tf.keras.layers.Dense(hidden_nodes, activation=\"relu\"))\n",
    "#     # Output layer\n",
    "#     model.add(tf.keras.layers.Dense(num_classes, activation=\"softmax\"))\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Parameters\n",
    "# hidden_layers = 4\n",
    "# hidden_nodes = 128\n",
    "# input_shape = 100  # Define the input shape according to your data\n",
    "# num_classes = 6  # Assuming you have 6 output classes\n",
    "\n",
    "# # Build the model\n",
    "# model = build_model(hidden_layers, hidden_nodes)\n",
    "\n",
    "# # Plot the model architecture\n",
    "# plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose this design because of question 4.1. In that question I noticed that\n",
    "by incresing layers and hidden nodes, model performance slowly increased. I\n",
    "thereby added more layers and hidden nodes, while reducing the learning rate to\n",
    "avoid overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
