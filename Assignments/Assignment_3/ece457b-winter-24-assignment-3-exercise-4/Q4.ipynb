{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Embedding,\n",
    "    Bidirectional,\n",
    "    Conv1D,\n",
    "    GlobalMaxPooling1D,\n",
    "    LSTM,\n",
    "    Attention,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Concatenate,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Count the occurrences of each genre\n",
    "genre_counts = train_data[\"genre\"].value_counts()\n",
    "\n",
    "\n",
    "# Plot the distribution of genres\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(genre_counts.index, genre_counts.values, color=\"skyblue\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Movie Genres\")\n",
    "plt.xticks(genre_counts.index, genre_counts.index.tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drama and documentary genres are the most common, with 5483 and 4861\n",
    "occurrences, respectively. Comedy is the third most common genre with 3896\n",
    "occurrences.Horror, thriller, and action genres have fewer occurrences compared\n",
    "to drama, documentary, and comedy, with 2104, 1568, and 1312 occurrences,\n",
    "respectively. The distribution of the dataset is imbalanced, with drama and\n",
    "documentary genres dominating the dataset, while horror, thriller, and action\n",
    "genres are underrepresented. This class imbalance can potentially affect the\n",
    "performance of machine learning models, particularly for genres with fewer\n",
    "occurrences, as the model may struggle to generalize well to these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "validation_data = pd.read_csv(\"validate.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=[\"genre\"])\n",
    "y_train = train_data[\"genre\"]\n",
    "X_val = validation_data.drop(columns=[\"genre\"])\n",
    "y_val = validation_data[\"genre\"]\n",
    "X_test = test_data.drop(columns=[\"ID\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_encoded), y=y_train_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize text data to get vocabulary size and maximum sequence length\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# max_seq_length = max([len(text.split()) for text in X_train])\n",
    "\n",
    "# # Set embedding dimension\n",
    "# embedding_dim = 100  # Example dimension, you can adjust it based on your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, hidden_nodes, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            hidden_nodes, activation=\"relu\", input_shape=(X_train.shape[1],)\n",
    "        )\n",
    "    )\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(hidden_nodes, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(len(label_encoder.classes_), activation=\"softmax\"))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    y_train_pred_data = model.predict(X_train)\n",
    "    y_train_pred = np.argmax(y_train_pred_data, axis=1)\n",
    "    y_val_pred_data = model.predict(X_val)\n",
    "    y_val_pred = np.argmax(y_val_pred_data, axis=1)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "    print(\"Accuracy Scores:\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        print(f\"{genre}: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structures = [\n",
    "    {\"hidden_layers\": 1, \"hidden_nodes\": 64},\n",
    "    # {\"hidden_layers\": 2, \"hidden_nodes\": 32},\n",
    "    # {\"hidden_layers\": 2, \"hidden_nodes\": 64},\n",
    "]\n",
    "\n",
    "for idx, structure in enumerate(model_structures, start=1):\n",
    "    hidden_layers = structure[\"hidden_layers\"]\n",
    "    hidden_nodes = structure[\"hidden_nodes\"]\n",
    "    learning_rate = 0.001\n",
    "    epochs = 100\n",
    "\n",
    "    print(\n",
    "        f\"\\nModel {idx}: Hidden Layers: {hidden_layers}, Hidden Nodes: {hidden_nodes}\"\n",
    "    )\n",
    "    model = build_model(hidden_layers, hidden_nodes, learning_rate)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_encoded,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val_encoded),\n",
    "        verbose=0,\n",
    "        # class_weight=dict(enumerate(class_weights))\n",
    "    )\n",
    "\n",
    "    plot_training_history(history, f\"Model {idx}: Training vs Validation Error\")\n",
    "\n",
    "    evaluate_model(model, X_train, y_train_encoded, X_val, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define custom neural network model\n",
    "# def build_custom_model(input_shape, num_classes):\n",
    "#     input_layer = Input(input_shape)\n",
    "#     x = Dense(128, activation=\"relu\")(input_layer)\n",
    "#     x = Dense(64, activation=\"relu\")(x)\n",
    "#     output_layer = tf.keras.layers.Dense((num_classes), activation=\"sigmoid\")(x)\n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model\n",
    "# model = build_custom_model(\n",
    "#     input_shape=(X_train.shape[1],), num_classes=len(label_encoder.classes_)\n",
    "# )\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     epochs=100,\n",
    "#     batch_size=256,\n",
    "#     validation_data=(X_val, y_val_encoded),\n",
    "#     verbose=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded, verbose=0)\n",
    "# val_loss, val_accuracy = model.evaluate(X_val, y_val_encoded, verbose=0)\n",
    "\n",
    "# # Predict on validation set\n",
    "# y_val_pred = np.argmax(model.predict(X_val), axis=-1)\n",
    "\n",
    "# # Compute class-wise accuracy\n",
    "# conf_matrix = confusion_matrix(y_val_encoded, y_val_pred)\n",
    "# class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# # Compute F1 score\n",
    "# f1_scores = []\n",
    "# for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "#     precision = (\n",
    "#         conf_matrix[label_encoder.transform([genre]), label_encoder.transform([genre])]\n",
    "#         / conf_matrix[:, label_encoder.transform([genre])].sum()\n",
    "#     )\n",
    "#     recall = (\n",
    "#         conf_matrix[label_encoder.transform([genre]), label_encoder.transform([genre])]\n",
    "#         / conf_matrix[label_encoder.transform([genre]), :].sum()\n",
    "#     )\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_scores.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print evaluation metrics\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "# print(\"Validation Accuracy:\", val_accuracy)\n",
    "# print(\"\\nClass-wise Accuracy:\")\n",
    "# for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "#     print(f\"{genre}: {acc}\")\n",
    "# print(\"\\nF1 Scores:\")\n",
    "# for genre, f1 in zip(label_encoder.classes_, f1_scores):\n",
    "#     print(f\"{genre}: {f1}\")\n",
    "\n",
    "# # Plot training history\n",
    "# plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training vs Validation Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    build_model_func,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    label_encoder,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # Instantiate the model\n",
    "    model = build_model_func(\n",
    "        input_shape=X_train.shape[1:], num_classes=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        # class_weight=dict(enumerate(class_weights)),\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = np.argmax(model.predict(X_val), axis=-1)\n",
    "\n",
    "    # Compute class-wise accuracy\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1_scores = []\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        precision = (\n",
    "            conf_matrix[\n",
    "                label_encoder.transform([genre]), label_encoder.transform([genre])\n",
    "            ]\n",
    "            / conf_matrix[:, label_encoder.transform([genre])].sum()\n",
    "        )\n",
    "        recall = (\n",
    "            conf_matrix[\n",
    "                label_encoder.transform([genre]), label_encoder.transform([genre])\n",
    "            ]\n",
    "            / conf_matrix[label_encoder.transform([genre]), :].sum()\n",
    "        )\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        print(f\"{genre}: {acc}\")\n",
    "    print(\"\\nF1 Scores:\")\n",
    "    for genre, f1 in zip(label_encoder.classes_, f1_scores):\n",
    "        print(f\"{genre}: {f1}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    submission_df = pd.DataFrame({\"ID\": test_data['ID'], \"label\": np.argmax(predictions, axis=1)})\n",
    "\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom neural network model\n",
    "def build_custom_model(input_shape, num_classes):\n",
    "    input_layer = Input(input_shape)\n",
    "    x = Dense(128, activation=\"relu\")(input_layer)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output_layer = tf.keras.layers.Dense((num_classes), activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    # predictions = model.predict(X_test)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    x = Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_lstm(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(32))(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_with_attention(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    attention = Attention()([x, x])\n",
    "    x = Dense(64, activation=\"relu\")(attention)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    cnn_branch = Conv1D(128, 5, activation=\"relu\")(cnn_branch)\n",
    "    cnn_branch = GlobalMaxPooling1D()(cnn_branch)\n",
    "    cnn_branch = Dense(64, activation=\"relu\")(cnn_branch)\n",
    "\n",
    "    # LSTM branch\n",
    "    lstm_branch = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    lstm_branch = Bidirectional(LSTM(64, return_sequences=True))(lstm_branch)\n",
    "    lstm_branch = Bidirectional(LSTM(32))(lstm_branch)\n",
    "    lstm_branch = Dense(64, activation=\"relu\")(lstm_branch)\n",
    "\n",
    "    # Concatenate both branches\n",
    "    concatenated = Concatenate()([cnn_branch, lstm_branch])\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(concatenated)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(\n",
    "    build_custom_model, X_train, y_train_encoded, X_val, y_val_encoded, label_encoder\n",
    ")\n",
    "# train_and_evaluate_model(\n",
    "#     build_cnn_model, X_train, y_train_encoded, X_val, y_val_encoded, label_encoder\n",
    "# )\n",
    "# train_and_evaluate_model(\n",
    "#     build_bidirectional_lstm,\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     X_val,\n",
    "#     y_val_encoded,\n",
    "#     label_encoder,\n",
    "# )\n",
    "# train_and_evaluate_model(\n",
    "#     build_lstm_with_attention,\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     X_val,\n",
    "#     y_val_encoded,\n",
    "#     label_encoder,\n",
    "# )\n",
    "# train_and_evaluate_model(\n",
    "#     build_ensemble_model,\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     X_val,\n",
    "#     y_val_encoded,\n",
    "#     label_encoder,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
