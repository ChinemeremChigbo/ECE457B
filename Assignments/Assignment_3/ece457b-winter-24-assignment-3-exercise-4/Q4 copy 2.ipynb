{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinemerem/Documents/GitHub/ECE457B/env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 601us/step - accuracy: 0.3230 - loss: 1.6156 - val_accuracy: 0.3509 - val_loss: 1.4786\n",
      "Epoch 2/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 516us/step - accuracy: 0.4683 - loss: 1.3839 - val_accuracy: 0.4412 - val_loss: 1.3090\n",
      "Epoch 3/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 541us/step - accuracy: 0.5219 - loss: 1.2452 - val_accuracy: 0.5663 - val_loss: 1.1812\n",
      "Epoch 4/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.5643 - loss: 1.1588 - val_accuracy: 0.5615 - val_loss: 1.1285\n",
      "Epoch 5/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.5886 - loss: 1.1055 - val_accuracy: 0.6084 - val_loss: 1.0708\n",
      "Epoch 6/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.5941 - loss: 1.0674 - val_accuracy: 0.5894 - val_loss: 1.0954\n",
      "Epoch 7/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.6050 - loss: 1.0402 - val_accuracy: 0.6132 - val_loss: 1.0508\n",
      "Epoch 8/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.6144 - loss: 1.0297 - val_accuracy: 0.6150 - val_loss: 1.0648\n",
      "Epoch 9/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.6259 - loss: 1.0015 - val_accuracy: 0.6138 - val_loss: 1.0683\n",
      "Epoch 10/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.6268 - loss: 1.0057 - val_accuracy: 0.6206 - val_loss: 1.0186\n",
      "Epoch 11/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.6288 - loss: 0.9754 - val_accuracy: 0.6256 - val_loss: 1.0220\n",
      "Epoch 12/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.6262 - loss: 0.9883 - val_accuracy: 0.6326 - val_loss: 1.0068\n",
      "Epoch 13/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594us/step - accuracy: 0.6295 - loss: 0.9790 - val_accuracy: 0.6325 - val_loss: 0.9983\n",
      "Epoch 14/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 539us/step - accuracy: 0.6386 - loss: 0.9545 - val_accuracy: 0.6165 - val_loss: 1.0197\n",
      "Epoch 15/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 520us/step - accuracy: 0.6398 - loss: 0.9473 - val_accuracy: 0.6358 - val_loss: 0.9870\n",
      "Epoch 16/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556us/step - accuracy: 0.6512 - loss: 0.9388 - val_accuracy: 0.6380 - val_loss: 0.9712\n",
      "Epoch 17/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 551us/step - accuracy: 0.6489 - loss: 0.9376 - val_accuracy: 0.6424 - val_loss: 0.9600\n",
      "Epoch 18/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6518 - loss: 0.9353 - val_accuracy: 0.6408 - val_loss: 0.9609\n",
      "Epoch 19/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - accuracy: 0.6544 - loss: 0.9209 - val_accuracy: 0.6251 - val_loss: 0.9785\n",
      "Epoch 20/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - accuracy: 0.6490 - loss: 0.9277 - val_accuracy: 0.6229 - val_loss: 0.9933\n",
      "Epoch 21/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 581us/step - accuracy: 0.6467 - loss: 0.9244 - val_accuracy: 0.6458 - val_loss: 0.9580\n",
      "Epoch 22/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step - accuracy: 0.6578 - loss: 0.9091 - val_accuracy: 0.5957 - val_loss: 1.0787\n",
      "Epoch 23/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 0.6481 - loss: 0.9284 - val_accuracy: 0.6457 - val_loss: 0.9559\n",
      "Epoch 24/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 0.6580 - loss: 0.9058 - val_accuracy: 0.6463 - val_loss: 0.9583\n",
      "Epoch 25/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 502us/step - accuracy: 0.6598 - loss: 0.9050 - val_accuracy: 0.6492 - val_loss: 0.9467\n",
      "Epoch 26/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.6576 - loss: 0.8911 - val_accuracy: 0.6478 - val_loss: 0.9452\n",
      "Epoch 27/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step - accuracy: 0.6652 - loss: 0.8981 - val_accuracy: 0.6241 - val_loss: 0.9843\n",
      "Epoch 28/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 503us/step - accuracy: 0.6592 - loss: 0.8927 - val_accuracy: 0.6393 - val_loss: 0.9572\n",
      "Epoch 29/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464us/step - accuracy: 0.6670 - loss: 0.8854 - val_accuracy: 0.6460 - val_loss: 0.9480\n",
      "Epoch 30/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 617us/step - accuracy: 0.6625 - loss: 0.8906 - val_accuracy: 0.6317 - val_loss: 0.9774\n",
      "Epoch 31/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - accuracy: 0.6695 - loss: 0.8788 - val_accuracy: 0.6277 - val_loss: 0.9793\n",
      "Epoch 32/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6618 - loss: 0.8847 - val_accuracy: 0.6271 - val_loss: 0.9835\n",
      "Epoch 33/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.6745 - loss: 0.8669 - val_accuracy: 0.6531 - val_loss: 0.9259\n",
      "Epoch 34/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 510us/step - accuracy: 0.6706 - loss: 0.8636 - val_accuracy: 0.6333 - val_loss: 0.9972\n",
      "Epoch 35/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 542us/step - accuracy: 0.6663 - loss: 0.8722 - val_accuracy: 0.6466 - val_loss: 0.9587\n",
      "Epoch 36/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 580us/step - accuracy: 0.6708 - loss: 0.8697 - val_accuracy: 0.6208 - val_loss: 1.0023\n",
      "Epoch 37/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490us/step - accuracy: 0.6645 - loss: 0.8701 - val_accuracy: 0.6359 - val_loss: 0.9734\n",
      "Epoch 38/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 534us/step - accuracy: 0.6691 - loss: 0.8742 - val_accuracy: 0.6469 - val_loss: 0.9369\n",
      "Epoch 39/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.6649 - loss: 0.8732 - val_accuracy: 0.6551 - val_loss: 0.9267\n",
      "Epoch 40/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 554us/step - accuracy: 0.6847 - loss: 0.8486 - val_accuracy: 0.6451 - val_loss: 0.9453\n",
      "Epoch 41/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6741 - loss: 0.8656 - val_accuracy: 0.6528 - val_loss: 0.9364\n",
      "Epoch 42/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 508us/step - accuracy: 0.6725 - loss: 0.8595 - val_accuracy: 0.6456 - val_loss: 0.9499\n",
      "Epoch 43/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.6720 - loss: 0.8638 - val_accuracy: 0.6374 - val_loss: 0.9558\n",
      "Epoch 44/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.6723 - loss: 0.8568 - val_accuracy: 0.6246 - val_loss: 0.9977\n",
      "Epoch 45/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 504us/step - accuracy: 0.6676 - loss: 0.8694 - val_accuracy: 0.6385 - val_loss: 0.9549\n",
      "Epoch 46/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.6717 - loss: 0.8685 - val_accuracy: 0.6470 - val_loss: 0.9551\n",
      "Epoch 47/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 0.6728 - loss: 0.8584 - val_accuracy: 0.6454 - val_loss: 0.9363\n",
      "Epoch 48/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step - accuracy: 0.6797 - loss: 0.8370 - val_accuracy: 0.6299 - val_loss: 0.9881\n",
      "Epoch 49/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493us/step - accuracy: 0.6780 - loss: 0.8481 - val_accuracy: 0.6305 - val_loss: 0.9782\n",
      "Epoch 50/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489us/step - accuracy: 0.6737 - loss: 0.8512 - val_accuracy: 0.6126 - val_loss: 1.0544\n",
      "Epoch 51/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475us/step - accuracy: 0.6761 - loss: 0.8544 - val_accuracy: 0.6324 - val_loss: 0.9820\n",
      "Epoch 52/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.6841 - loss: 0.8326 - val_accuracy: 0.6373 - val_loss: 0.9680\n",
      "Epoch 53/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 536us/step - accuracy: 0.6768 - loss: 0.8470 - val_accuracy: 0.6491 - val_loss: 0.9352\n",
      "Epoch 54/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.6799 - loss: 0.8485 - val_accuracy: 0.6190 - val_loss: 1.0231\n",
      "Epoch 55/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 526us/step - accuracy: 0.6756 - loss: 0.8424 - val_accuracy: 0.6458 - val_loss: 0.9517\n",
      "Epoch 56/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 537us/step - accuracy: 0.6813 - loss: 0.8367 - val_accuracy: 0.6519 - val_loss: 0.9411\n",
      "Epoch 57/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.6735 - loss: 0.8544 - val_accuracy: 0.6373 - val_loss: 0.9721\n",
      "Epoch 58/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 575us/step - accuracy: 0.6737 - loss: 0.8441 - val_accuracy: 0.6267 - val_loss: 0.9958\n",
      "Epoch 59/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 571us/step - accuracy: 0.6807 - loss: 0.8380 - val_accuracy: 0.6242 - val_loss: 0.9898\n",
      "Epoch 60/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - accuracy: 0.6796 - loss: 0.8331 - val_accuracy: 0.6528 - val_loss: 0.9355\n",
      "Epoch 61/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 592us/step - accuracy: 0.6819 - loss: 0.8281 - val_accuracy: 0.6491 - val_loss: 0.9423\n",
      "Epoch 62/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 564us/step - accuracy: 0.6759 - loss: 0.8397 - val_accuracy: 0.6320 - val_loss: 0.9948\n",
      "Epoch 63/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 528us/step - accuracy: 0.6798 - loss: 0.8345 - val_accuracy: 0.6440 - val_loss: 0.9508\n",
      "Epoch 64/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step - accuracy: 0.6775 - loss: 0.8372 - val_accuracy: 0.6311 - val_loss: 0.9839\n",
      "Epoch 65/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - accuracy: 0.6799 - loss: 0.8400 - val_accuracy: 0.6445 - val_loss: 0.9441\n",
      "Epoch 66/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.6817 - loss: 0.8263 - val_accuracy: 0.6372 - val_loss: 0.9762\n",
      "Epoch 67/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515us/step - accuracy: 0.6854 - loss: 0.8257 - val_accuracy: 0.6427 - val_loss: 0.9754\n",
      "Epoch 68/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.6837 - loss: 0.8241 - val_accuracy: 0.5868 - val_loss: 1.1356\n",
      "Epoch 69/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.6801 - loss: 0.8347 - val_accuracy: 0.6283 - val_loss: 0.9937\n",
      "Epoch 70/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step - accuracy: 0.6936 - loss: 0.8110 - val_accuracy: 0.6417 - val_loss: 0.9660\n",
      "Epoch 71/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 524us/step - accuracy: 0.6891 - loss: 0.8244 - val_accuracy: 0.6459 - val_loss: 0.9661\n",
      "Epoch 72/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.6841 - loss: 0.8235 - val_accuracy: 0.6511 - val_loss: 0.9288\n",
      "Epoch 73/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485us/step - accuracy: 0.6873 - loss: 0.8095 - val_accuracy: 0.6321 - val_loss: 0.9766\n",
      "Epoch 74/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 522us/step - accuracy: 0.6807 - loss: 0.8271 - val_accuracy: 0.6450 - val_loss: 0.9466\n",
      "Epoch 75/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 497us/step - accuracy: 0.6882 - loss: 0.8159 - val_accuracy: 0.6319 - val_loss: 0.9747\n",
      "Epoch 76/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 531us/step - accuracy: 0.6889 - loss: 0.8149 - val_accuracy: 0.6227 - val_loss: 1.0272\n",
      "Epoch 77/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.6848 - loss: 0.8182 - val_accuracy: 0.6262 - val_loss: 1.0118\n",
      "Epoch 78/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 523us/step - accuracy: 0.6865 - loss: 0.8233 - val_accuracy: 0.6415 - val_loss: 0.9777\n",
      "Epoch 79/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 527us/step - accuracy: 0.6855 - loss: 0.8233 - val_accuracy: 0.6511 - val_loss: 0.9365\n",
      "Epoch 80/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495us/step - accuracy: 0.6871 - loss: 0.8161 - val_accuracy: 0.6459 - val_loss: 0.9597\n",
      "Epoch 81/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 473us/step - accuracy: 0.6827 - loss: 0.8251 - val_accuracy: 0.6391 - val_loss: 0.9690\n",
      "Epoch 82/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 492us/step - accuracy: 0.6898 - loss: 0.8165 - val_accuracy: 0.6318 - val_loss: 0.9900\n",
      "Epoch 83/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457us/step - accuracy: 0.6915 - loss: 0.8061 - val_accuracy: 0.6480 - val_loss: 0.9415\n",
      "Epoch 84/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - accuracy: 0.6875 - loss: 0.8084 - val_accuracy: 0.6481 - val_loss: 0.9474\n",
      "Epoch 85/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 525us/step - accuracy: 0.6889 - loss: 0.8100 - val_accuracy: 0.6367 - val_loss: 0.9830\n",
      "Epoch 86/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6906 - loss: 0.8082 - val_accuracy: 0.6317 - val_loss: 0.9706\n",
      "Epoch 87/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.6821 - loss: 0.8207 - val_accuracy: 0.6395 - val_loss: 0.9784\n",
      "Epoch 88/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 514us/step - accuracy: 0.6918 - loss: 0.8045 - val_accuracy: 0.6480 - val_loss: 0.9553\n",
      "Epoch 89/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477us/step - accuracy: 0.6884 - loss: 0.8122 - val_accuracy: 0.6451 - val_loss: 0.9558\n",
      "Epoch 90/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6986 - loss: 0.7924 - val_accuracy: 0.6172 - val_loss: 1.0523\n",
      "Epoch 91/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 499us/step - accuracy: 0.6944 - loss: 0.8021 - val_accuracy: 0.6364 - val_loss: 0.9935\n",
      "Epoch 92/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 511us/step - accuracy: 0.6928 - loss: 0.8036 - val_accuracy: 0.6431 - val_loss: 0.9683\n",
      "Epoch 93/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 521us/step - accuracy: 0.6860 - loss: 0.8217 - val_accuracy: 0.6526 - val_loss: 0.9437\n",
      "Epoch 94/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519us/step - accuracy: 0.6933 - loss: 0.7978 - val_accuracy: 0.6315 - val_loss: 0.9784\n",
      "Epoch 95/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518us/step - accuracy: 0.6883 - loss: 0.8091 - val_accuracy: 0.6403 - val_loss: 0.9722\n",
      "Epoch 96/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 506us/step - accuracy: 0.6957 - loss: 0.7912 - val_accuracy: 0.6509 - val_loss: 0.9527\n",
      "Epoch 97/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549us/step - accuracy: 0.6958 - loss: 0.7903 - val_accuracy: 0.6488 - val_loss: 0.9467\n",
      "Epoch 98/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 530us/step - accuracy: 0.6934 - loss: 0.7943 - val_accuracy: 0.6257 - val_loss: 1.0420\n",
      "Epoch 99/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 496us/step - accuracy: 0.6867 - loss: 0.8174 - val_accuracy: 0.6382 - val_loss: 0.9885\n",
      "Epoch 100/100\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484us/step - accuracy: 0.6883 - loss: 0.8032 - val_accuracy: 0.6497 - val_loss: 0.9429\n",
      "\u001b[1m601/601\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244us/step\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218us/step\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224us/step\n",
      "Accuracy Scores:\n",
      "Training Accuracy: 0.6990220557636287\n",
      "Validation Accuracy: 0.6497196616934334\n",
      "Test Accuracy: 0.21913902879407013\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.55      0.52       657\n",
      "           1       0.58      0.68      0.63      2241\n",
      "           2       0.81      0.80      0.81      2716\n",
      "           3       0.62      0.63      0.62      3012\n",
      "           4       0.68      0.69      0.68      1102\n",
      "           5       0.43      0.13      0.20       795\n",
      "\n",
      "    accuracy                           0.65     10523\n",
      "   macro avg       0.60      0.58      0.58     10523\n",
      "weighted avg       0.64      0.65      0.64     10523\n",
      "\n",
      "\u001b[1m329/329\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "validation_data = pd.read_csv(\"validate.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Prepare data\n",
    "X_train = train_data.drop(columns=[\"genre\"])\n",
    "y_train = train_data[\"genre\"]\n",
    "X_val = validation_data.drop(columns=[\"genre\"])\n",
    "y_val = validation_data[\"genre\"]\n",
    "X_test = test_data.drop(columns=[\"ID\"])\n",
    "\n",
    "# Normalize input features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "\n",
    "# Define label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_encoded), y=y_train_encoded\n",
    ")\n",
    "\n",
    "\n",
    "# Define model\n",
    "def build_model(\n",
    "    input_shape, output_classes, hidden_layers, hidden_nodes, learning_rate\n",
    "):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(hidden_nodes, activation=\"relu\", input_shape=input_shape)\n",
    "    )\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(hidden_nodes, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(output_classes, activation=\"softmax\"))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    y_train_pred = np.argmax(model.predict(X_train), axis=1)\n",
    "    y_val_pred = np.argmax(model.predict(X_val), axis=1)\n",
    "    y_val_test = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    test_accuracy = accuracy_score(y_val, y_val_test)\n",
    "\n",
    "    print(\"Accuracy Scores:\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "input_shape = (X_train.shape[1],)\n",
    "output_classes = len(label_encoder.classes_)\n",
    "hidden_layers = 1\n",
    "hidden_nodes = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# Build and train the model\n",
    "model = build_model(\n",
    "    input_shape, output_classes, hidden_layers, hidden_nodes, learning_rate\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train_encoded,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val_encoded),\n",
    "    verbose=1,\n",
    "    # class_weight=dict(enumerate(class_weights)),\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, X_train, y_train_encoded, X_val, y_val_encoded)\n",
    "\n",
    "# Make predictions on test set\n",
    "test_predictions = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\"ID\": test_data[\"ID\"], \"label\": test_predictions})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m genre_counts \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Plot the distribution of genres\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mbar(genre_counts\u001b[38;5;241m.\u001b[39mindex, genre_counts\u001b[38;5;241m.\u001b[39mvalues, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskyblue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenre\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Count the occurrences of each genre\n",
    "genre_counts = train_data[\"genre\"].value_counts()\n",
    "\n",
    "\n",
    "# Plot the distribution of genres\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(genre_counts.index, genre_counts.values, color=\"skyblue\")\n",
    "plt.xlabel(\"Genre\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Movie Genres\")\n",
    "plt.xticks(genre_counts.index, genre_counts.index.tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drama and documentary genres are the most common, with 5483 and 4861\n",
    "occurrences, respectively. Comedy is the third most common genre with 3896\n",
    "occurrences.Horror, thriller, and action genres have fewer occurrences compared\n",
    "to drama, documentary, and comedy, with 2104, 1568, and 1312 occurrences,\n",
    "respectively. The distribution of the dataset is imbalanced, with drama and\n",
    "documentary genres dominating the dataset, while horror, thriller, and action\n",
    "genres are underrepresented. This class imbalance can potentially affect the\n",
    "performance of machine learning models, particularly for genres with fewer\n",
    "occurrences, as the model may struggle to generalize well to these classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = pd.read_csv(\"validate.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.drop(columns=[\"genre\"])\n",
    "y_train = train_data[\"genre\"]\n",
    "X_val = validation_data.drop(columns=[\"genre\"])\n",
    "y_val = validation_data[\"genre\"]\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize input features\n",
    "# scaler = StandardScaler()\n",
    "# X_train_normalized = scaler.fit_transform(X_train)\n",
    "# X_val_normalized = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", classes=np.unique(y_train_encoded), y=y_train_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize text data to get vocabulary size and maximum sequence length\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "# max_seq_length = max([len(text.split()) for text in X_train])\n",
    "\n",
    "# # Set embedding dimension\n",
    "# embedding_dim = 100  # Example dimension, you can adjust it based on your preference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden_layers, hidden_nodes, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            hidden_nodes, activation=\"relu\", input_shape=(X_train.shape[1],)\n",
    "        )\n",
    "    )\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(tf.keras.layers.Dense(hidden_nodes, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(len(label_encoder.classes_), activation=\"softmax\"))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    y_train_pred_data = model.predict(X_train)\n",
    "    y_train_pred = np.argmax(y_train_pred_data, axis=1)\n",
    "    y_val_pred_data = model.predict(X_val)\n",
    "    y_val_pred = np.argmax(y_val_pred_data, axis=1)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "    print(\"Accuracy Scores:\")\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        print(f\"{genre}: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 1: Hidden Layers: 1, Hidden Nodes: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinemerem/Documents/GitHub/ECE457B/env/lib/python3.12/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_structures = [\n",
    "    {\"hidden_layers\": 1, \"hidden_nodes\": 64},\n",
    "    # {\"hidden_layers\": 2, \"hidden_nodes\": 32},\n",
    "    # {\"hidden_layers\": 2, \"hidden_nodes\": 64},\n",
    "]\n",
    "\n",
    "for idx, structure in enumerate(model_structures, start=1):\n",
    "    hidden_layers = structure[\"hidden_layers\"]\n",
    "    hidden_nodes = structure[\"hidden_nodes\"]\n",
    "    learning_rate = 0.001\n",
    "    epochs = 100\n",
    "\n",
    "    print(\n",
    "        f\"\\nModel {idx}: Hidden Layers: {hidden_layers}, Hidden Nodes: {hidden_nodes}\"\n",
    "    )\n",
    "    model = build_model(hidden_layers, hidden_nodes, learning_rate)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train_encoded,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val_encoded),\n",
    "        verbose=0,\n",
    "        # class_weight=dict(enumerate(class_weights))\n",
    "    )\n",
    "\n",
    "    plot_training_history(history, f\"Model {idx}: Training vs Validation Error\")\n",
    "\n",
    "    evaluate_model(model, X_train, y_train_encoded, X_val, y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define custom neural network model\n",
    "# def build_custom_model(input_shape, num_classes):\n",
    "#     input_layer = Input(input_shape)\n",
    "#     x = Dense(128, activation=\"relu\")(input_layer)\n",
    "#     x = Dense(64, activation=\"relu\")(x)\n",
    "#     output_layer = tf.keras.layers.Dense((num_classes), activation=\"sigmoid\")(x)\n",
    "#     model = Model(inputs=input_layer, outputs=output_layer)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the model\n",
    "# model = build_custom_model(\n",
    "#     input_shape=(X_train.shape[1],), num_classes=len(label_encoder.classes_)\n",
    "# )\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     epochs=100,\n",
    "#     batch_size=256,\n",
    "#     validation_data=(X_val, y_val_encoded),\n",
    "#     verbose=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# train_loss, train_accuracy = model.evaluate(X_train, y_train_encoded, verbose=0)\n",
    "# val_loss, val_accuracy = model.evaluate(X_val, y_val_encoded, verbose=0)\n",
    "\n",
    "# # Predict on validation set\n",
    "# y_val_pred = np.argmax(model.predict(X_val), axis=-1)\n",
    "\n",
    "# # Compute class-wise accuracy\n",
    "# conf_matrix = confusion_matrix(y_val_encoded, y_val_pred)\n",
    "# class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "# # Compute F1 score\n",
    "# f1_scores = []\n",
    "# for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "#     precision = (\n",
    "#         conf_matrix[label_encoder.transform([genre]), label_encoder.transform([genre])]\n",
    "#         / conf_matrix[:, label_encoder.transform([genre])].sum()\n",
    "#     )\n",
    "#     recall = (\n",
    "#         conf_matrix[label_encoder.transform([genre]), label_encoder.transform([genre])]\n",
    "#         / conf_matrix[label_encoder.transform([genre]), :].sum()\n",
    "#     )\n",
    "#     f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "#     f1_scores.append(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print evaluation metrics\n",
    "# print(\"Training Accuracy:\", train_accuracy)\n",
    "# print(\"Validation Accuracy:\", val_accuracy)\n",
    "# print(\"\\nClass-wise Accuracy:\")\n",
    "# for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "#     print(f\"{genre}: {acc}\")\n",
    "# print(\"\\nF1 Scores:\")\n",
    "# for genre, f1 in zip(label_encoder.classes_, f1_scores):\n",
    "#     print(f\"{genre}: {f1}\")\n",
    "\n",
    "# # Plot training history\n",
    "# plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "# plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training vs Validation Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    build_model_func,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    label_encoder,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "):\n",
    "    # Instantiate the model\n",
    "    model = build_model_func(\n",
    "        input_shape=X_train.shape[1:], num_classes=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        # class_weight=dict(enumerate(class_weights)),\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train, verbose=0)\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_val_pred = np.argmax(model.predict(X_val), axis=-1)\n",
    "\n",
    "    # Compute class-wise accuracy\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    class_wise_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1_scores = []\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        precision = (\n",
    "            conf_matrix[\n",
    "                label_encoder.transform([genre]), label_encoder.transform([genre])\n",
    "            ]\n",
    "            / conf_matrix[:, label_encoder.transform([genre])].sum()\n",
    "        )\n",
    "        recall = (\n",
    "            conf_matrix[\n",
    "                label_encoder.transform([genre]), label_encoder.transform([genre])\n",
    "            ]\n",
    "            / conf_matrix[label_encoder.transform([genre]), :].sum()\n",
    "        )\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"Training Accuracy:\", train_accuracy)\n",
    "    print(\"Validation Accuracy:\", val_accuracy)\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for genre, acc in zip(label_encoder.classes_, class_wise_accuracy):\n",
    "        print(f\"{genre}: {acc}\")\n",
    "    print(\"\\nF1 Scores:\")\n",
    "    for genre, f1 in zip(label_encoder.classes_, f1_scores):\n",
    "        print(f\"{genre}: {f1}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.plot(history.history[\"loss\"], label=\"Train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    submission_df = pd.DataFrame({\"ID\": test_data['ID'], \"label\": np.argmax(predictions, axis=1)})\n",
    "\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom neural network model\n",
    "def build_custom_model(input_shape, num_classes):\n",
    "    input_layer = Input(input_shape)\n",
    "    x = Dense(128, activation=\"relu\")(input_layer)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output_layer = tf.keras.layers.Dense((num_classes), activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    # predictions = model.predict(X_test)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    x = Conv1D(128, 5, activation=\"relu\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bidirectional_lstm(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(32))(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_with_attention(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    attention = Attention()([x, x])\n",
    "    x = Dense(64, activation=\"relu\")(attention)\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ensemble_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # CNN branch\n",
    "    cnn_branch = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    cnn_branch = Conv1D(128, 5, activation=\"relu\")(cnn_branch)\n",
    "    cnn_branch = GlobalMaxPooling1D()(cnn_branch)\n",
    "    cnn_branch = Dense(64, activation=\"relu\")(cnn_branch)\n",
    "\n",
    "    # LSTM branch\n",
    "    lstm_branch = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(input_layer)\n",
    "    lstm_branch = Bidirectional(LSTM(64, return_sequences=True))(lstm_branch)\n",
    "    lstm_branch = Bidirectional(LSTM(32))(lstm_branch)\n",
    "    lstm_branch = Dense(64, activation=\"relu\")(lstm_branch)\n",
    "\n",
    "    # Concatenate both branches\n",
    "    concatenated = Concatenate()([cnn_branch, lstm_branch])\n",
    "    output_layer = Dense(num_classes, activation=\"softmax\")(concatenated)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate_model(\n",
    "    build_custom_model, X_train, y_train_encoded, X_val, y_val_encoded, label_encoder\n",
    ")\n",
    "# train_and_evaluate_model(\n",
    "#     build_cnn_model, X_train, y_train_encoded, X_val, y_val_encoded, label_encoder\n",
    "# )\n",
    "# train_and_evaluate_model(\n",
    "#     build_bidirectional_lstm,\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     X_val,\n",
    "#     y_val_encoded,\n",
    "#     label_encoder,\n",
    "# )\n",
    "# train_and_evaluate_model(\n",
    "#     build_lstm_with_attention,\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     X_val,\n",
    "#     y_val_encoded,\n",
    "#     label_encoder,\n",
    "# )\n",
    "# train_and_evaluate_model(\n",
    "#     build_ensemble_model,\n",
    "#     X_train,\n",
    "#     y_train_encoded,\n",
    "#     X_val,\n",
    "#     y_val_encoded,\n",
    "#     label_encoder,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
