{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Squared Loss (or l2 loss)\n",
    "    - (a) Whether the loss can be used for classification, regression, or both\n",
    "      - Squared loss can be used for both classification and regression\n",
    "    - (b) The assumptions, if any, that the loss function makes over the outputs\n",
    "      of the model\n",
    "      - Assumes that the output of the model is continuous\n",
    "      - Assumes the errors between the predicted and true values are normally\n",
    "        distributed with a mean of zero and a constant variance\n",
    "    - (c) One advantage and disadvantage of employing the loss\n",
    "    function – e.g., assuming we are operating in the proper regime\n",
    "    (classification/regression) for the given loss function, what\n",
    "    characteristics of the data/model/learning does the loss function implicitly\n",
    "    impose?\n",
    "      - Advantage:\n",
    "        - Differentiability and convexity leads to smooth optimization landscapes, ideal for gradient-based optimization \n",
    "      - Disadvantage:\n",
    "        - Outliers have a disproportionately large impact on the loss\n",
    "\n",
    "2. Mean Absolute Error (or l1 loss)\n",
    "   - (a) Whether the loss can be used for classification, regression, or both\n",
    "     - Mean absolute error loss can be used for both classification and regression\n",
    "   - (b) The assumptions, if any, that the loss function makes over the outputs\n",
    "    of the model\n",
    "      - Assumes the output of the model is continuous\n",
    "    - (c) One advantage and disadvantage of employing the loss\n",
    "    function – e.g., assuming we are operating in the proper regime\n",
    "    (classification/regression) for the given loss function, what\n",
    "    characteristics of the data/model/learning does the loss function implicitly\n",
    "    impose?\n",
    "      - Advantage\n",
    "        - Robust to outliers\n",
    "      - Disadvantage\n",
    "        - Non-differentiability leads to difficulties in gradient-based optimization\n",
    "\n",
    "3. Binary Cross-Entropy Error\n",
    "    - (a) Whether the loss can be used for classification, regression, or both\n",
    "      - Binary Cross-Entropy Error is used for classification\n",
    "    - (b) The assumptions, if any, that the loss function makes over the outputs\n",
    "    of the model\n",
    "      - Assumes that the data follows a binary distribution with independent observations\n",
    "    - (c) One advantage and disadvantage of employing the loss\n",
    "    function – e.g., assuming we are operating in the proper regime\n",
    "    (classification/regression) for the given loss function, what\n",
    "    characteristics of the data/model/learning does the loss function implicitly\n",
    "    impose?\n",
    "    - Advantage\n",
    "      - Penalizes the model more strongly for confidently incorrect predictions\n",
    "    - Disadvantage\n",
    "      - Can be sensitive to class imbalance and outliers\n",
    "4. Huber Loss Error\n",
    "    - (a) Whether the loss can be used for classification, regression, or both\n",
    "      - Huber Loss can be used for both regression and classification\n",
    "    - (b) The assumptions, if any, that the loss function makes over the outputs\n",
    "    of the model\n",
    "      - Assumes a heavy tailed distribution with constant variance\n",
    "    - (c) One advantage and disadvantage of employing the loss\n",
    "    function – e.g., assuming we are operating in the proper regime\n",
    "    (classification/regression) for the given loss function, what\n",
    "    characteristics of the data/model/learning does the loss function implicitly\n",
    "    impose?\n",
    "    - Advantage\n",
    "      - Can be less sensitive to outliers by penalizing large errors linearly\n",
    "        (dependent of the choice of δ)\n",
    "    - Disadvantage\n",
    "      - Performance is sensitive to choosing an appropriate value for\n",
    "        hyperparameter, δ, (may require cross-validation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
